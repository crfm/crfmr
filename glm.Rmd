---
title: "GLM - think of title name"
output: 
  html_document: 
    fig_height: 3
    fig_width: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE)
```

# Preamble

> NOTE: This document is only partially completed

In this exercise the power of combining functions from the dplyr and ggplot2 packages (and then some) as a data exploration tool to reveal potential patterns and trends is demonstrated.

It is expected that readers are already familiar with the basics of data [manipulation](dplyr.html) and [plotting](ggplot2.html).

The data we are going to use is the regional catch and effort flying fish data that was used as a part of the [UNU-FTP stock assessment course](http://fishvice.hafro.is/doku.php/crfm:start) that was held some years ago. It data contains observation of catch and effort by year, month, country and vessel type.

The most interesting parameter we may want to focus on is the catch per unit effort. This is because it is often proposed that such data may be a proxy measure of the actual biomass index. In analytical models that use time series data (stock production model or the more data demanding length and/or age based models) the cpue index is often assumed to be linearly related to stock biomass or abundance (green line in the schematic graph below) through an equation sometimes referred to as "the link model"

$CPUE = qX$

where cpue can be either in mass or in numbers and equivalently the X either the stock biomass or stock numbers. The $q$ (often called catchability is estimated within the model).

The linear relationship (read: "The model assumption") is often suspected to not hold (read: "Is wrong"), particular if the index (read: cpue) is base on fisheries dependent data. The reason in the latter case is that fisherman's behavior is driven by getting has high catch as possible with the least amount of effort (cost of fishing often plays a role). So even though the stock may e.g. be declining fishermen will change behavior by whatever means in order hold up the catch per unit effort.

Add text to introduce: 

$CPUE = qX^b$

In other words the fisherman's objective is not estimating changes in stock size over time - it only us fisheries scientist that are sometimes daring enough to make that assumption. Often muttering at the same time "Given the data that one has this is the best one can do".

```{r, echo = FALSE}
library(tidyverse)
data_frame(biomass = c(0:1000),
           linear = 0.001 * biomass,
           hyper = 0.001 * biomass^3,
           uber  = 0.001 * biomass^0.3) %>% 
  gather(variable, value, linear:uber) %>% 
  group_by(variable, biomass) %>% 
  ungroup() %>% 
  group_by(variable) %>% 
  mutate(biomass = biomass/max(biomass),
         value = value/max(value)) %>% 
  ggplot(aes(biomass, value, colour = variable)) +
  theme_bw() +
  geom_line() +
  scale_x_continuous("Biomass", NULL) +
  scale_y_continuous("CPUE", NULL) +
  labs(colour = NULL) +
  theme(legend.position = c(0.15, 0.82))
```

But enough of a preamble, our main objective here is to learn some more R, including standardization of catch per unit effort data using GLM.


"They do not address any basic problems with cpue as an abundance index, such as hyperstability or hyperdepletion"

# Getting the data into R

Once done you should have the flyfish Excel sheet in your current working directory (recall that to get information of the current working directory one can use the `getwd` command. If you open the workbook in Excel and go to sheet "flyfish" you see that the data we are interested in reading into R starts in row 3 and column "F" (number 6) and ends column "K" (number 6). Some may note that there is also data in column "L" (CPUE). We can omit them in the importing step because we can derive from other variables ("Weight (kg)" and "Trip" internally in R. Since the data is only a section of the worksheet we use the functions in the `XLConnect` package using the following code:

```{r}
library(XLConnect)
library(tidyverse)
library(lubridate)
library(stringr)
library(broom)
df <- read.csv("http://www.hafro.is/~einarhj/crfmr/data-raw/flyingfish.csv",
               stringsAsFactors = FALSE)
```

Lets see what we got:
```{r}
glimpse(df)
```

So we have `r nrow(df)` observations. Take note that the variable type of Year, Month, Weight..kg. and Trips are numerical values (labelled as `<dbl>` above) as expected.
The column names are not to my liking (I want to minimize keyboard work in the code that follows) so I change the column names:
```{r}
names(df) <- c("year", "month", "country", "vessel", "catch", "effort")
```

And because we are going to be most interested in the catch per unit effort data we might as well generate that variable:

```{r}
df <-
  df %>% 
  mutate(cpue = catch/effort)
```

# A tiny introduction to "date" format in R

There are two columns that refer to time, year and month. Because I want later to plot data along time I might as well set up the proper date variable. Here I use the `ymd` function from the lubridate package (for further reading check the introductory [vignette](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html)). Since there is no specific "day" in the data I just use the first day of each month as a dummy constant.

```{r}
df <- 
  df %>% 
  mutate(cpue = catch/effort,
         date = ymd(paste(year, month, 1, sep = "-")))
```

If you now take a glimpse at the data you notice that the type for the date column is labelled as `<date>` (i.e. as expected).


# An introduction to GLM

> TODO: Add text here to explain model and code

Catch per unit effort is in many cases the only index available to inform on stock trends. Raw cpue may, as we have seen, be influenced by factors such as fishing season, fishing area, vessel and gear type. If these change over the time (years) the trend in the raw cpue index may be reflecting changes in these factors rather than being indicative of changes in biomass. Take an imaginary simple case where there was a gradual change in the season when fishing effort (and hence sampling) occurs. E.g. from being more or less the same in all months in the beginning towards being focused only when the highest catch rates occur in terminal years. All else being equal a simple annual mean of all the observations would result in annual index that would increase over time.

Standardizing cpue with generalized linear models (GLMs) provides one way to remove some of these effects, resulting in an index that hopefully more accurately reflects changes in the true biomass. It however does not alleviate all problems such imprecise measure of true effort and the issue of "hyperstability" discussed at the beginning of this section.

If the data is already in a tidy format setting up the R-code for the model is relatively easy:

```{r}
df <-
  df %>% 
  mutate(lcatch = log(catch),
         leffort = log(effort),
         year = factor(year),
         month = factor(month))

m <- glm(formula = lcatch ~ leffort + year + month + country + vessel,
     data = df)
```

In the above model the variable we are trying to "explain" is the catch (on a log-scale) as a function of the effort (on a log scale), year, month, country and vessel. Lets see what we got:

```{r}
m
```

This object is no longer a familiar dataframe frame but of a class that is called '"glm" "lm"' (try running `class(m)`. Besides the coefficients there are a bunch of statistical measures in the output that I try to explain.

The __deviance__ is a measure of goodness of fit of a generalized linear model, the higher the value the worse the fit. The "Null Deviance" shows how well the response variable is predicted by a model that includes only the intercept (grand mean). For our example, we have a value of 3700 with 588 degrees of freedom (that is the total number of observations minus 1, try `nrow(df)-1`). Including the independent variables (effort, year, month, country and vessel) decreased the deviance to 337.1 with 553 degrees of freedom, a significant reduction in deviance. I.e the deviance was reduced by `r 3700-337.1` with a loss of `r 588 - 553` degrees of freedom.

The __AIC__ (Akaike Information Criterion) provides a method for assessing the quality of your model through comparison of related models.  It’s based on the Deviance, but penalizes you for making the model more complicated.  Much like adjusted R-squared, it’s intent is to prevent you from including irrelevant predictors.

However, unlike adjusted R-squared, the number itself is not meaningful. If you have more than one similar candidate models (where all of the variables of the simpler model occur in the more complex models), then you should select the model that has the smallest AIC. We will look at that later on.

We can get the coefficient statistics with the `tidy` function from the __broom__-package:

```{r}
res <- tidy(m)
res
```

We thus have a dataframe with the following columns:

* __term__: these are related explanatory variables we included in the analysis (log effort, year, month, vessel, country)
* __estimates__: These are the coefficient estimates (see later).
* __std.error__: The standard error of the coefficients
* __statistic__: The t-statistics
* __p.value__: The probability value that indicate significance

Because the coefficient values (estimate) and the associated error are on a log scale lets rescale them:
```{r}
res <-
  res %>%
  mutate(mean = exp(estimate),
         lower.ci = exp(estimate - 2 * std.error),
         upper.ci = exp(estimate + 2 * std.error))
```

Now even though the information we are most interested is the year factor lets visualize the other factors because they are also of interest. Lets start with months:

```{r}
var <- "month"
res %>% 
  filter(str_detect(term, var)) %>% 
  mutate(term = str_replace(term, var, ""),
         term = as.integer(term)) %>% 
  ggplot(aes(term, mean)) +
  theme_bw() +
  geom_hline(yintercept = 1, colour = "grey") +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymin = lower.ci,
                     ymax = upper.ci)) +
  scale_x_continuous(name = "Month", breaks = c(2:12)) +
  scale_y_continuous(name = "Index")
```

You first notice that the first month is not included. That is because the model by default "sets" that to one (actually zero on the log scale). Here I have just included a horizontal line to represent that value. To interpret this plot you can think of the response values representing the relative catch per unit effort by month where the year, vessel and country factor have been taken into account. The error bars represent the 95% confidence interval (approximately). Notice e.g. that the error bars for the month of 2, 3, 5, 6 and 12 "overlap" with one (the horizontal line). That means that mean cpue for those months is not significantly different from the first month. So the broad seasonal patterns are that the high catch rates occur in the fishing season December to June followed by the low catch rate season from July through November.

```{r}
# Food for thought:
res %>% 
  filter(str_detect(term, var)) %>% 
  mutate(term = str_replace(term, var, ""),
         term = as.integer(term)) %>% 
  ggplot(aes(term, std.error)) +
  geom_point() +
  expand_limits(y = 0)
```

```{r, echo = FALSE}
x1 <- round(res$mean[res$term == "vesselIceboats"],2)
x2 <- paste0(round(res$lower.ci[res$term == "vesselIceboats"],2),
             " - ",
             round(res$upper.ci[res$term == "vesselIceboats"],2))
```

Equivalently we can do this for the vessel type. Since we have only two vessel type we only obtain the mean and std.error estimates for the Iceboats (the Dayboats are then set to one). The mean is `r x1`and the 95% confidence interval is `r x2`. Ergo the Iceboats have approximately 8 times higher catch rates than the Dayboats all else being equal. And the difference is significant.

When we look at the country term we get:
```{r, echo = FALSE}
res %>% 
  filter(str_detect(term, "country")) %>% 
  select(term, lower.ci, mean, upper.ci, p.value)
```

I.e all else being equal the catch rates for St. Lucia are lower and the catch rates for Tobago higher than that observed in Barbados, both being significantly different. Take note that this does not mean that the Tobago fleet is the most efficient rather it may mean that the local density in the waters may be higher, that the true effort is higher, etc. Here I lack expertise/knowledge of these flying fish fisheries. But it may be of interest to dig a little deeper into the reason for the above observation.

Now finally the term that is normally of interest, the year factors:

```{r}
var <- "year"
res %>% 
  filter(str_detect(term, var)) %>% 
  mutate(term = str_replace(term, var, ""),
         term = as.integer(term)) %>% 
  ggplot(aes(term, mean)) +
  theme_bw() +
  geom_hline(yintercept = 1, colour = "grey") +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymin = lower.ci,
                     ymax = upper.ci)) +
  labs(x = NULL, y = "Index") +
  expand_limits(y = 0)
```

If we were to assume that the standardize cpue index is a good indicator of the true biomass ($CPUE = qB$) we can at minimum conclude with some reasonable confidence that the long term effect of human removals from this stock has not had any detrimental effect on the stock development.

# GLM - stepwise addition of variables

> TODO: Add text here to explain approach

```{r}
m1 <- glm(formula = lcatch ~ leffort + year, data = df)
r1 <-
  m1 %>% 
  tidy() %>% 
  mutate(model = "1. + year")
m2 <- glm(formula = lcatch ~ leffort + year + vessel, data = df)
r2 <-
  m2 %>% 
  tidy() %>% 
  mutate(model = "2. + vessel")
m3 <-  glm(formula = lcatch ~ leffort + year + vessel + month, data = df)
r3 <-
  m3 %>%
  tidy() %>% 
  mutate(model = "3. + month")
m4 <- lm(formula = lcatch ~ leffort + year + month + country + vessel, data = df)
r4 <-
  m4 %>%
  tidy() %>% 
  mutate(model = "4. + country")
  
d <-
  bind_rows(r1, r2) %>% 
  bind_rows(r3) %>% 
  bind_rows(r4) %>% 
  mutate(mean = exp(estimate),
         lower.ci = exp(estimate - 2 * std.error),
         upper.ci = exp(estimate + 2 * std.error)) %>% 
  filter(str_detect(term, "year")) %>% 
  mutate(term = str_replace(term, "year", ""),
         term = as.integer(term))
d %>%
  ggplot(aes(term, mean, colour = model)) +
  geom_line() +
  geom_point() +
  labs(x = NULL, y = "Standardized cpue")
  
d %>% 
  ggplot(aes(term, std.error, colour = model)) + 
  geom_line() +
  geom_point() +
  labs(x = NULL, y = "Coefficent of variation")
```


```{r, echo = FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

```{r, echo = FALSE}
# Leftovers

model_full <-  
  lm(formula = catch ~ effort + year + month + country + vessel,
     data = ff2)
summary(model_full)
r0 <-
  model_full %>% 
  tidy() %>%
  separate(term, c("variable", "value")) %>% 
  mutate(est = exp(estimate),
         lower = exp(estimate - 2 * std.error),
         upper = exp(estimate + 2 * std.error))
df2 <- tibble(variable = c("year", "month", "country", "vessel"),
             value = c("1988", "1", "Barbados", "Dayboats"),
             est = c(1, 1, 1, 1))
r0 <-
  r0 %>% 
  bind_rows(df2)
r0 %>% 
  filter(variable == "year") %>% 
  mutate(year = as.integer(value)) %>% 
  ggplot(aes(year, est)) + 
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  geom_line()
r0 %>% 
  filter(variable == "month") %>% 
  mutate(month = as.integer(value)) %>% 
  ggplot(aes(month, est)) + 
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:12))
r0 %>% 
  filter(variable == "country") %>% 
  ggplot(aes(value, est)) +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  coord_flip()
r0 %>% 
  filter(variable == "vessel") %>% 
  ggplot(aes(value, est)) +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  coord_flip()



```

```{r, eval = FALSE, echo = FALSE}
# LEFTOVERS
df %>% 
  ggplot(aes(effort, catch, colour = country)) +
  geom_point() +
  facet_wrap(~ vessel, scale = "free")

df %>% 
  filter(vessel == "Dayboats") %>% 
  ggplot(aes(log(effort), log(catch), colour = country)) +
  theme_bw() +
  geom_point(alpha = 0.6, size = 1) +
  geom_smooth(method = "lm")

# food for later thought
df %>% 
  filter(vessel == "Dayboats",
         effort <= 195) %>% 
  ggplot(aes(log(effort), log(catch), colour = country)) +
  theme_bw() +
  geom_point(alpha = 0.6, size = 1) +
  geom_smooth(method = "lm")

df %>% 
  filter(vessel == "Dayboats") %>% 
  ggplot(aes(country, cpue)) +
  theme_bw() +
  geom_boxplot()
df %>% 
  filter(vessel == "Dayboats") %>% 
  ggplot(aes(country, cpue)) +
  theme_bw() +
  stat_summary(fun.data = "mean_cl_boot", colour = "red", size = 1)


df %>% 
  group_by(year, country, vessel) %>% 
  summarise(catch = sum(catch)/1e3) %>% 
  ggplot(aes(year, catch)) +
  geom_col() +
  facet_wrap(~ vessel + country)#, scale = "free_y")



m1 <-  
  lm(formula = log(catch)~log(effort) + factor(year) + factor(month) + vessel,
     data = df)
m2 <-  
  lm(formula = log(catch)~log(effort) + factor(year) + factor(month),
     data = df)
m3 <-  
  lm(formula = log(catch)~log(effort) + factor(year),
     data = df)
extractAIC(model_full)
extractAIC(m1)
extractAIC(m2)
extractAIC(m3)

d <-
  df %>% 
  group_by(year, country, vessel) %>% 
  summarise(catch = sum(catch),
            effort = sum(effort),
            cpue.mean = mean(cpue),
            cpue.std = sd(cpue),
            Fproxy = catch/cpue.mean)

d %>% 
  ggplot(aes(year, Fproxy, colour = vessel)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~ country, scale = "free_y")

# https://www.r-bloggers.com/standard-deviation-vs-standard-error/

df %>% 
  ggplot(aes(factor(month), cpue)) +
  geom_boxplot()

p <- 
  df %>% 
  ggplot() +
  scale_x_continuous(breaks = c(1:12))
p +
  stat_summary(aes(month, cpue, group = month),
               fun.data = "mean_cl_boot", colour = "red", size = 1)

# https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html

#bootnls <- mtcars %>% bootstrap(100) %>%
#    do(tidy(nls(mpg ~ k / wt + b, ., start=list(k=1, b=0))))

one_boot <- function(d) {
  df %>% 
  group_by(month) %>% 
  sample_frac(size = 0.5) %>%
  summarise(m = mean(cpue))
}


x <- list()
for(i in 1:5000) {
  x[[i]] <- one_boot(df) 
}
x2 <-
  x %>% 
  bind_rows() %>% 
  group_by(month) %>% 
  summarise(med2    = mean(m),
            ci.low  = quantile(m, probs = 0.025),
            ci.high = quantile(m, probs = 0.975))

p +
  stat_summary(aes(month, cpue, group = month),
               fun.data = "mean_cl_boot", colour = "red", size = 1) +
  geom_point(data = x2, aes(month, med2)) +
  geom_linerange(data = x2, aes(month, ymin = ci.low, ymax = ci.high))



d <- 
  df %>% 
  group_by(month) %>% 
  summarise(n.obs = n(),
            m = mean(cpue),
            std.dev = sd(cpue),       # the standard deviation
            sem = std.dev/n.obs,      # the standard error
            ci.lower = m - 2 * sem,   # lower 95% confidence interval (approximate)
            ci.upper = m + 2 * sem)   # upper 95% confidence interval (approximate)
glimpse(d)
d %>% 
  ggplot(aes(month, m)) +
  geom_errorbar(aes(ymin = m - std.dev, ymax = m + std.dev)) + 
  geom_errorbar(aes(ymin = ci.lower, ymax = ci.upper))


It depends. If the message you want to carry is about the spread and variability of the data, then standard deviation is the metric to use. If you are interested in the precision of the means or in comparing and testing differences between means then standard error is your metric. Of course deriving confidence intervals around your data (using standard deviation) or the mean (using standard error) requires your data to be normally distributed. Bootstrapping is an option to derive confidence intervals in cases when you are doubting the normality of your data.

http://www.theanalysisfactor.com/generalized-linear-models-glm-r-part4/
  
  http://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/icesjms/69/1/10.1093/icesjms/fsr174/2/fsr174.pdf?Expires=1486226713&Signature=fdD9QlEi4Rd8C3wKQCEHPdsNo1bcd8-t7HNFA9muBjARaQfG5DF1qW6S1HhMxlbxfq0ppXOu9sMWL9ccRrRiXlEAR-8xFTMsAWTiWiksaa1TPxW-YDCXhhPXB67AIZ1WLVSuTKaHV2t3IiwGFUBBNlftHzKkGdY-pQ~z~UPmsXEEpS2aIUHych6srKuOefZp156YCVpYRbqPxAJD2EPZuboTpBlS~TiE0T5Ej-5tzAGhjdKtkyhjDO4TqKtZKdknRifmkjJPkkMFt9Alxb7yZDojkUeWhoJuJJRC5iDL0T3ggasyrdhD5yElGJMh8-GJ2DbPxosedzSH99uYUyB50Q__&Key-Pair-Id=APKAIUCZBIA4LVPAVW3Q
```
